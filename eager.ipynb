{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you enable eager execution, operations execute immediately and \n",
    "return their values to Python without requiring a Session.run(). \n",
    "For example, to multiply two matrices together, we write this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfe.enable_eager_execution()\n",
    "\n",
    "x = [[2.]]\n",
    "\n",
    "m = tf.matmul(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 4.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic models can be built with Python flow control. \n",
    "Here's an example of the Collatz conjecture using TensorFlow’s arithmetic operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float64)\n",
      "tf.Tensor(3.0, shape=(), dtype=float64)\n",
      "tf.Tensor(10.0, shape=(), dtype=float64)\n",
      "tf.Tensor(5.0, shape=(), dtype=float64)\n",
      "tf.Tensor(16.0, shape=(), dtype=float64)\n",
      "tf.Tensor(8.0, shape=(), dtype=float64)\n",
      "tf.Tensor(4.0, shape=(), dtype=float64)\n",
      "tf.Tensor(2.0, shape=(), dtype=float64)\n",
      "tf.Tensor(1.0, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(12)\n",
    "counter = 0\n",
    "while not tf.equal(a, 1):\n",
    "    if tf.equal(a % 2, 0):\n",
    "        a = a / 2\n",
    "    else:\n",
    "        a = 3 * a + 1\n",
    "    print(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most TensorFlow users are interested in automatic differentiation. \n",
    "Because different operations can occur during each call, we record all forward operations to a tape, \n",
    "which is then played backwards when computing gradients. \n",
    "After we've computed the gradients, we discard the tape. \n",
    "If you’re familiar with the autograd package, the API is very similar. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "[<tf.Tensor: id=112, shape=(), dtype=float32, numpy=6.0>]\n"
     ]
    }
   ],
   "source": [
    "def square(x):\n",
    "    return tf.multiply(x, x)\n",
    "\n",
    "grad = tfe.gradients_function(square)\n",
    "\n",
    "print(square(3.))    # [9.]\n",
    "print(grad(3.))      # [6.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients_function call takes a Python function square() as an argument and \n",
    "returns a Python callable that computes the partial derivatives of square() with respect to its inputs. \n",
    "So, to get the derivative of square() at 3.0, invoke grad(3.0), which is 6. \n",
    "The same gradients_function call can be used to get the second derivative of square:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=140, shape=(), dtype=float32, numpy=2.0>]\n"
     ]
    }
   ],
   "source": [
    "gradgrad = tfe.gradients_function(lambda x: grad(x)[0])\n",
    "\n",
    "print(gradgrad(3.))  # [2.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we noted, control flow can cause different operations to run, such as in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=150, shape=(), dtype=float32, numpy=1.0>]\n",
      "[<tf.Tensor: id=161, shape=(), dtype=float32, numpy=-1.0>]\n"
     ]
    }
   ],
   "source": [
    "def abs(x):\n",
    "    return x if x > 0. else -x\n",
    "\n",
    "grad = tfe.gradients_function(abs)\n",
    "\n",
    "print(grad(2.0))  # [1.]\n",
    "print(grad(-2.0)) # [-1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users may want to define custom gradients for an operation, or for a function. \n",
    "This may be useful for multiple reasons, including providing a more efficient or \n",
    "more numerically stable gradient for a sequence of operations. Here is an example that illustrates \n",
    "the use of custom gradients. Let's start by looking at the function log(1 + ex), \n",
    "which commonly occurs in the computation of cross entropy and log likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=173, shape=(), dtype=float32, numpy=0.5>]\n",
      "[<tf.Tensor: id=185, shape=(), dtype=float32, numpy=nan>]\n"
     ]
    }
   ],
   "source": [
    "def log1pexp(x):\n",
    "    return tf.log(1 + tf.exp(x))\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# The gradient computation works fine at x = 0.\n",
    "print(grad_log1pexp(0.))\n",
    "# [0.5]\n",
    "# However it returns a `nan` at x = 100 due to numerical instability.\n",
    "print(grad_log1pexp(100.))\n",
    "# [nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a custom gradient for the above function that analytically simplifies the gradient expression. \n",
    "Notice how the gradient function implementation below reuses an expression (tf.exp(x)) \n",
    "that was computed during the forward pass, \n",
    "making the gradient computation more efficient by avoiding redundant computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=198, shape=(), dtype=float32, numpy=0.5>]\n",
      "[<tf.Tensor: id=211, shape=(), dtype=float32, numpy=1.0>]\n"
     ]
    }
   ],
   "source": [
    "@tfe.custom_gradient\n",
    "def log1pexp(x):\n",
    "    e = tf.exp(x)\n",
    "    def grad(dy):\n",
    "        return dy * (1 - 1 / (1 + e))\n",
    "    return tf.log(1 + e), grad\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# Gradient at x = 0 works as before.\n",
    "print(grad_log1pexp(0.))\n",
    "# [0.5]\n",
    "# And now gradient computation at x=100 works as well.\n",
    "print(grad_log1pexp(100.))\n",
    "# [1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be organized in classes. \n",
    "Here's a model class that creates a (simple) two layer network that can classify \n",
    "the standard MNIST handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(tfe.Network):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.layer1 = self.track_layer(tf.layers.Dense(units=10))\n",
    "        self.layer2 = self.track_layer(tf.layers.Dense(units=10))\n",
    "    def call(self, input):\n",
    "        \"\"\"Actually runs the model.\"\"\"\n",
    "        result = self.layer1(input)\n",
    "        result = self.layer2(result)\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend using the classes (not the functions) in tf.layers since they create and contain model parameters (variables). Variable lifetimes are tied to the lifetime of the layer objects, so be sure to keep track of them. Why are we using tfe.Network? A Network is a container for layers and is a tf.layer.Layer itself, allowing Network objects to be embedded in other Network objects. It also contains utilities to assist with inspection, saving, and restoring. Even without training the model, we can imperatively call it and inspect the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 784)\n",
      "tf.Tensor([[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]], shape=(1, 1, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Let's make up a blank input image\n",
    "model = MNISTModel()\n",
    "batch = tf.zeros([1, 1, 784])\n",
    "print(batch.shape)\n",
    "# (1, 1, 784)\n",
    "result = model(batch)\n",
    "print(result)\n",
    "# tf.Tensor([[[ 0.  0., ...., 0.]]], shape=(1, 1, 10), dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not need any placeholders or sessions. The first time we pass in the input, the sizes of the layers’ parameters are set. To train any model, we define a loss function to optimize, calculate gradients, and use an optimizer to update the variables. First, here's a loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, our training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b870633e1989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplicit_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "for (x, y) in tfe.Iterator(dataset):\n",
    "    grads = tfe.implicit_gradients(loss_function)(model, x, y)\n",
    "    optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implicit_gradients() calculates the derivatives of loss_function with respect to all the TensorFlow variables used during its computation. We can move computation to a GPU the same way we’ve always done with TensorFlow:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a05e59724cda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/gpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    for (x, y) in tfe.Iterator(dataset):\n",
    "        optimizer.minimize(lambda: loss_function(model, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: We're shortcutting storing our loss and directly calling the optimizer.minimize, but you could also use the apply_gradients() method above; they are equivalent.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow bleeding-edge",
   "language": "python",
   "name": "tf-nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
